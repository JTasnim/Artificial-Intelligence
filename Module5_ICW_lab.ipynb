{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99211b4e-ee5c-4e7c-96df-37c22ee56753",
      "metadata": {
        "id": "99211b4e-ee5c-4e7c-96df-37c22ee56753"
      },
      "source": [
        "# Lab Assignment 1: Understanding Markov Decision Processes and Value Iteration\n",
        "\n",
        "## Objective\n",
        "In this lab, you will explore the fundamentals of Markov Decision Processes (MDPs) using a transportation problem. You will implement and analyze the value iteration algorithm to find the optimal policy for reaching a destination.\n",
        "\n",
        "## Background\n",
        "The `TransportationMDP` class models a scenario where an agent moves from location 1 to location \\(N\\) (set to 27). The agent can either \"walk\" (move to \\(state + 1\\)) or take a \"tram\" (move to \\(2 \\times state\\) with success probability 0.5, or stay in the same state with failure probability 0.5). Walking costs 1 unit, and taking the tram also costs 1 unit. The goal is to minimize the total cost to reach \\(N\\).\n",
        "\n",
        "## Tasks\n",
        "1. **Run the Provided Code**\n",
        "   - Execute the code with \\(N = 27\\). Observe the output of the `valueIteration` function.\n",
        "   - Note: Comment out `os.system('clear')` if it doesnâ€™t work on your system.\n",
        "\n",
        "2. **Modify Parameters**\n",
        "   - Change `failProb` to 0.2 and rerun. Record the policy for states 1, 5, 10, and 20.\n",
        "   - Change `walkCost` to 2 and rerun with `failProb = 0.5`. Record the policy for the same states.\n",
        "\n",
        "3. **Implement a Utility Function**\n",
        "   - Add `printPolicyAndValues` to print a table of state, value, and action.\n",
        "\n",
        "4. **Analyze Convergence**\n",
        "   - Modify `valueIteration` to track iterations until convergence.\n",
        "\n",
        "## Questions\n",
        "1. What does the optimal policy suggest about the trade-off between walking and taking the tram when `failProb = 0.5`? How does this change when `failProb = 0.2`?\n",
        "2. Why does increasing `walkCost` to 2 affect the policy in certain states?\n",
        "3. How does the discount factor (currently 1.0) influence the solution?\n",
        "4. Is the policy always deterministic in this MDP? Why or why not?\n",
        "\n",
        "## Deliverables\n",
        "- Submit your modified code.\n",
        "- Provide a short report (1-2 pages) answering the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "132e2516-a7c2-43e7-ada9-cfef26e8ea34",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "132e2516-a7c2-43e7-ada9-cfef26e8ea34",
        "outputId": "bf392e1e-893e-4951-ba88-0a9f0ff8c742"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: 'walk',\n",
              "  2: 'walk',\n",
              "  3: 'tram',\n",
              "  4: 'walk',\n",
              "  5: 'walk',\n",
              "  6: 'tram',\n",
              "  7: 'walk',\n",
              "  8: 'walk',\n",
              "  9: 'walk',\n",
              "  10: 'walk',\n",
              "  11: 'walk',\n",
              "  12: 'walk',\n",
              "  13: 'tram',\n",
              "  14: 'walk',\n",
              "  15: 'walk',\n",
              "  16: 'walk',\n",
              "  17: 'walk',\n",
              "  18: 'walk',\n",
              "  19: 'walk',\n",
              "  20: 'walk',\n",
              "  21: 'walk',\n",
              "  22: 'walk',\n",
              "  23: 'walk',\n",
              "  24: 'walk',\n",
              "  25: 'walk',\n",
              "  26: 'walk',\n",
              "  27: None},\n",
              " {1: -9.999999999882107,\n",
              "  2: -8.999999999938439,\n",
              "  3: -7.9999999999678835,\n",
              "  4: -7.99999999999477,\n",
              "  5: -6.999999999997328,\n",
              "  6: -5.999999999998636,\n",
              "  7: -8.99999999999909,\n",
              "  8: -7.999999999999545,\n",
              "  9: -6.999999999999773,\n",
              "  10: -5.999999999999886,\n",
              "  11: -4.999999999999943,\n",
              "  12: -3.9999999999999716,\n",
              "  13: -2.999999999999986,\n",
              "  14: -13.0,\n",
              "  15: -12.0,\n",
              "  16: -11.0,\n",
              "  17: -10.0,\n",
              "  18: -9.0,\n",
              "  19: -8.0,\n",
              "  20: -7.0,\n",
              "  21: -6.0,\n",
              "  22: -5.0,\n",
              "  23: -4.0,\n",
              "  24: -3.0,\n",
              "  25: -2.0,\n",
              "  26: -1.0,\n",
              "  27: 0})"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "class TransportationMDP(object):\n",
        "    walkCost = 1\n",
        "    tramCost = 1\n",
        "    failProb = 0.5\n",
        "\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "\n",
        "    def startState(self):\n",
        "        return 1\n",
        "\n",
        "    def isEnd(self, state):\n",
        "        return state == self.N\n",
        "\n",
        "    def actions(self, state):\n",
        "        results = []\n",
        "        if state + 1 <= self.N:\n",
        "            results.append('walk')\n",
        "        if 2 * state <= self.N:\n",
        "            results.append('tram')\n",
        "        return results\n",
        "\n",
        "    def succProbReward(self, state, action):\n",
        "        results = []\n",
        "        if action == 'walk':\n",
        "            results.append((state + 1, 1, -self.walkCost))\n",
        "        elif action == 'tram':\n",
        "            results.append((state, self.failProb, -self.tramCost))\n",
        "            results.append((2 * state, 1 - self.failProb, -self.tramCost))\n",
        "        return results\n",
        "\n",
        "    def discount(self):\n",
        "        return 1.0\n",
        "\n",
        "    def states(self):\n",
        "        return list(range(1, self.N + 1))\n",
        "\n",
        "def valueIteration(mdp):\n",
        "    V = {}\n",
        "    pi = {}\n",
        "    def Q(state, action):\n",
        "        return sum(prob * (reward + mdp.discount() * V.get(newState, 0))\n",
        "                   for newState, prob, reward in mdp.succProbReward(state, action))\n",
        "\n",
        "    for state in mdp.states():\n",
        "        V[state] = 0\n",
        "\n",
        "    while True:\n",
        "        newV = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                newV[state], pi[state] = 0, None\n",
        "            else:\n",
        "                newV[state], pi[state] = max((Q(state, action), action) for action in mdp.actions(state))\n",
        "            # print(f'{state}: {newV[state]} {pi[state]}')\n",
        "\n",
        "        if max(abs(V[state] - newV[state]) for state in mdp.states()) < 1e-10:\n",
        "            break\n",
        "        V = newV\n",
        "    return pi,V\n",
        "\n",
        "mdp = TransportationMDP(N=27)\n",
        "valueIteration(mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial output shows a policy that primarily chooses walking for most states, with occasional tram  choices at states like 3, 6, and 13.\n",
        "\n",
        "Trade-off between walking and tram:\n",
        "\n",
        "with failProb=0.5: Policy prefers walking in most cases as tram has 50% failure chance\n",
        "\n",
        "with failProb=0.2: Policy shifts toward more tram usage as it becomes more reliable\n",
        "\n",
        "Effect of increasing walkCost to 2:\n",
        "\n",
        "makes walking relatively more expensive, causing policy to prefer tram in more states\n",
        "\n",
        "particularly affects states where tram can make significant progress\n",
        "\n",
        "Discount factor influence:\n",
        "\n",
        "current discount=1.0 means no future discounting - all future costs are equally important\n",
        "\n",
        "\n",
        "Policy determinism:\n",
        "\n",
        "the policy is always deterministic in this MDP because:\n",
        "\n",
        "value iteration produces deterministic policies for this type of MDP\n",
        "\n",
        "at each state, one action strictly dominates the other in expected value\n",
        "\n",
        "no action has exactly equal expected values in any state\n"
      ],
      "metadata": {
        "id": "RKAWpb_AOhLQ"
      },
      "id": "RKAWpb_AOhLQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code models an MDP for a simple transportation scenario:\n",
        "\n",
        "you are at position state on a number line from 1 to N.\n",
        "\n",
        "your goal is to reach state N as cheaply as possible.\n",
        "\n",
        "At each state, you can either:\n",
        "\n",
        "walk: Move from state to state + 1, always succeeds, with a fixed cost.\n",
        "\n",
        "tram: Tries to go from state to 2 * state. But this action may fail: with probability failProb, you stay in the same place, paid the tram's cost anyway; with probability 1 - failProb, you successfully move to 2 * state.\n",
        "\n"
      ],
      "metadata": {
        "id": "4OW4r4O3Qq2m"
      },
      "id": "4OW4r4O3Qq2m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Attributes\n",
        "\n",
        "walkCost = 1\n",
        "\n",
        "tramCost = 1\n",
        "\n",
        "failProb = 0.5"
      ],
      "metadata": {
        "id": "MxDUYd9RUPgj"
      },
      "id": "MxDUYd9RUPgj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "__init__(self, N)\n",
        "stores the maximum state (N), the destination.\n",
        "\n",
        "startState(self)\n",
        "starting point on the line (1).\n",
        "\n",
        "isEnd(self, state)\n",
        "are you at your goal (state N)?\n",
        "\n",
        "actions(self, state)\n",
        "returns possible moves:\n",
        "\n",
        "'walk' (to next state)\n",
        "\n",
        "'tram' (to 2*state, if you don't overshoot N)\n",
        "only gives actions that are possible from given state.\n",
        "\n",
        "succProbReward(self, state, action)\n",
        "for a given action, returns a list of (new_state, probability, reward) tuples:\n",
        "\n",
        "walk: Deterministic, always goes to state + 1, cost is -walkCost.\n",
        "\n",
        "tram: With 50% chance, stay (failProb, cost -tramCost). Otherwise (1-failProb), go to 2*state (-tramCost).\n",
        "\n",
        "discount(self)\n",
        "return 1.0, i.e., no discounting (future and current rewards equally valued).\n",
        "\n",
        "states(self)\n",
        "return list of all possible states."
      ],
      "metadata": {
        "id": "Z8CBO2ugUVvf"
      },
      "id": "Z8CBO2ugUVvf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration Algorithm\n",
        "\n",
        "this is an iterative dynamic programming algorithm that computes the optimal value function V and policy pi for reaching state N in the cheapest way.\n",
        "\n",
        "Initialize V: set V=0 for all states.\n",
        "\n",
        "While not converged:\n",
        "\n",
        "For each state:\n",
        "\n",
        "if at goal, V[state] = 0.\n",
        "\n",
        "else, for every possible action, compute the expected value using transition model and the running value function.\n",
        "\n",
        "Take the action with the maximum expected value; that's the greedy policy.\n",
        "\n",
        "Convergence: When the value function stops changing much (tolerance < 1e-10).\n",
        "\n",
        "Returns: The optimal policy and value function.\n",
        "\n"
      ],
      "metadata": {
        "id": "9MPIK5Q6UpBm"
      },
      "id": "9MPIK5Q6UpBm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the MDP for N=27.\n",
        "\n",
        "Run value iteration.\n",
        "\n",
        "Output is the optimal action at each state (policy) and minimum expected cost to reach N from each state (value function)."
      ],
      "metadata": {
        "id": "2BoLfVsNU3L0"
      },
      "id": "2BoLfVsNU3L0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}