{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b550bc0a-9657-48a6-92bc-808b609956eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06030e89-e1b5-421e-8af6-952a000ec507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid = [[0,  0, 1],\\\n",
    "        [0, -1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fecb2d7-88f6-4f49-b777-edd6783b7545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "137731f9-ea5a-4078-8272-1b782017d104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q = np.zeros((2,3,4)) # rows x cols x actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9c533f0-9fec-46b5-b300-2a771bbd064d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "V = {} # dictioary to store the return for each (state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a4ba15-b478-4bfb-9756-85a12263cfa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c13b997-0588-40ce-b16b-c9b3758e1526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    r,c = state # r is row, c is column number\n",
    "    if action =='up':\n",
    "        r = max(0, r-1)\n",
    "    if action =='down':\n",
    "        r = min(1, r+1)\n",
    "    if action =='left':\n",
    "        c = max(0, c-1)\n",
    "    if action =='right':\n",
    "        c = min(2, c+1)  \n",
    "    return (r,c)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7adc0569-7d5c-42ed-8bae-26daa2a3c91c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_state((0,1), 'down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cbe67774-4238-4229-b100-19432b1ebc42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Q-table:\n",
      " [[[-0.24072987 -0.55452494 -0.2560369  -0.03740747]\n",
      "  [-0.03842835 -1.         -0.22582771  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.2981654  -0.55510095 -0.47618509 -1.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    episode=[]\n",
    "    state = (0,0)\n",
    "    while grid[state[0]][state[1]]==0:\n",
    "        idx = np.random.randint(4)\n",
    "        action = actions[idx]\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = grid[next_state[0]][ next_state[1]]\n",
    "        episode.append((state, action, reward))\n",
    "        state= next_state\n",
    "\n",
    "    #print(episode)    \n",
    "    G = 0\n",
    "\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + gamma*G  \n",
    "        state_action = tuple(state) + (action,)\n",
    "        if state_action not in V:\n",
    "            V[state_action]=[]\n",
    "        V[state_action].append(G)\n",
    "        idx = actions.index(action)\n",
    "        \n",
    "        Q[state[0]][state[1]][idx]=np.mean(V[state_action])\n",
    "print(\"Monte Carlo Q-table:\\n\", Q)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68b3202b-3df8-40a3-881e-e2cf694c2c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Q-table Using Utility:\n",
      " [[[-0.29363462 -0.56999525 -0.36810418 -0.34573226]\n",
      "  [ 0.13477342 -1.         -0.31915079  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.29647478 -0.4546927  -0.48827501 -1.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "eta = 0.1  # Learning rate: how fast Q updates\n",
    "np.random.seed(42)\n",
    "Q = np.zeros((2,3,4)) # rows x cols x actions\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    episode=[]\n",
    "    state = (0,0)\n",
    "    while grid[state[0]][state[1]]==0:\n",
    "        idx = np.random.randint(4)\n",
    "        action = actions[idx]\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = grid[next_state[0]][ next_state[1]]\n",
    "        episode.append((state, action, reward))\n",
    "        state= next_state\n",
    "\n",
    "    #print(episode)    \n",
    "    G = 0\n",
    "\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + gamma*G  \n",
    "        idx = actions.index(action)\n",
    "        Q[state[0]][state[1]][idx] +=\\\n",
    "        -eta * (Q[state[0]][state[1]][idx]-G)\n",
    "print(\"Monte Carlo Q-table Using Utility:\\n\", Q)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c9dd6-807a-4571-ab03-1a55efe92a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6d361c5-8ee7-4975-b168-1abf8f40fcae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA Q-table:\n",
      " [[[-0.2179384  -0.49029948 -0.1516475  -0.23227311]\n",
      "  [ 0.00148482 -1.         -0.16555867  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.14464207 -0.69375646 -0.50408981 -1.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "eta = 0.1  # Learning rate: how fast Q updates\n",
    "gamma = 0.9 # discount rate\n",
    "np.random.seed(42)\n",
    "grid = [[0, 0, 1], [0, -1, 0]]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "Q = np.zeros((2, 3, 4))\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    state = (0,0) # initial state\n",
    "    action_idx = np.random.randint(4) # initial action\n",
    "    while grid[state[0]][state[1]]==0:\n",
    "        \n",
    "        action = actions[action_idx]\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = grid[next_state[0]][ next_state[1]]\n",
    "        next_action_idx = np.random.randint(4)\n",
    "        \n",
    "        \n",
    "        Q[state[0]][state[1]][action_idx] += eta * (\n",
    "            reward + gamma * Q[next_state[0]][next_state[1]][next_action_idx]\\\n",
    "            - Q[state[0]][state[1]][action_idx]\n",
    "        )\n",
    "        state = next_state\n",
    "        action_idx = next_action_idx  # Follow the policy\n",
    "        \n",
    "print(\"SARSA Q-table:\\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e61f2214-8bf3-420b-b6f8-039ec5738aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA Q-table with epsilon-greedy algorithm:\n",
      " [[[ 0.61123594  0.58454454  0.60276723  0.74699543]\n",
      "  [ 0.80070854 -0.9774716   0.62482031  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.7157109   0.10045814  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid = [[0, 0, 1], [0, -1, 0]]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "Q = np.zeros((2, 3, 4))\n",
    "alpha = 0.1  # Learning rate: how fast Q updates\n",
    "gamma = 0.9\n",
    "epsilon = 0.1  # Exploration: 10% random actions\n",
    "episodes = 1000\n",
    "\n",
    "def choose_action(state, Q, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(4)  # Random action\n",
    "    return np.argmax(Q[state[0]][state[1]])  # Best action\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = [0, 0]\n",
    "    action_idx = choose_action(state, Q, epsilon)  # Initial action\n",
    "    while grid[state[0]][state[1]] == 0:\n",
    "        next_state = get_next_state(state, actions[action_idx])\n",
    "        reward = grid[next_state[0]][next_state[1]]\n",
    "        next_action_idx = choose_action(next_state, Q, epsilon)  # Next action\n",
    "        # SARSA update\n",
    "        Q[state[0]][state[1]][action_idx] += alpha * (\n",
    "            reward + gamma * Q[next_state[0]][next_state[1]][next_action_idx]\\\n",
    "            - Q[state[0]][state[1]][action_idx]\n",
    "        )\n",
    "        state = next_state\n",
    "        action_idx = next_action_idx  # Follow the policy\n",
    "\n",
    "print(\"SARSA Q-table with epsilon-greedy algorithm:\\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390e9fc-058a-43ab-995c-01bee29d0925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
